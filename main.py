# -*- coding: utf-8 -*-
"""dbFieldClassification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EcE_QcWNA8qQ225Nz1sCzbWfhxoHoGsK
"""

# from google.colab import drive
# drive.mount('/content/drive/')

"""# Imports

"""

# Commented out IPython magic to ensure Python compatibility.
# Matplot
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns

# DataFrame
import pandas as pd

# Scikit-learn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score

# Keras
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Activation, Dense, Dropout, Embedding, Flatten , MaxPooling1D, LSTM, SpatialDropout1D,GlobalMaxPool1D,Bidirectional
from keras import utils
from keras.callbacks import ReduceLROnPlateau, EarlyStopping , ModelCheckpoint
from keras.models import load_model

# Word2vec
import gensim

# Utility
import re
import numpy as np

# nltk
import nltk

#----------- if you need wordnet run follow line
# nltk.download('wordnet')


def process_data(data):
    output = ''
    # old_df = pd.read_csv("dataset.csv")
    df = data

    # print('size of old dataset:',len(old_df))
    print('size of new dataset:',len(df))
    output += 'size of new dataset: ' + str(len(df)) + '<br><br>'

    df.head(60)

    df = df.filter(['field_after_change','is_closed'])
    # old_df = old_df.filter(['field_after_change','is_closed'])

    # df_temp = df[df.is_closed == 1]
    # df_temp.to_csv('drive/MyDrive/pouya/fields_are_just_closed.csv')

    """### remove nan from dataset"""

    df = df.dropna()
    df = df.reset_index(drop=True)

    df['is_closed'].sum()

    #shuffeling
    df = df.sample(frac=1).reset_index(drop=True)

    # plt.figure(figsize = (8, 6))
    # sns.countplot(df['is_closed'])
    # plt.show()

    df

    """## text cleaning and preprocessing"""

    from nltk.stem.porter import PorterStemmer
    stemmer = PorterStemmer()

    def preprocess_text(sentence):
      # Remove punctuations and numbers
      sentence = re.sub('[^a-zA-Z]', ' ', sentence)

      # Single character removal
      sentence = re.sub(r"\s+[a-zA-Z]\s+", ' ', sentence)

      # Removing multiple spaces
      sentence = re.sub(r'\s+', ' ', sentence)

      # convert to lower case
      tokens = [w.lower() for w in sentence.split(' ')]
      # stemming (coming -> come)
      # tokens = [stemmer.stem(w) for w in tokens]
      sentence = ' '.join(tokens)
      return sentence

    # from nltk.stem import WordNetLemmatizer

    # lemmatizer = WordNetLemmatizer()

    # print("rocks :", lemmatizer.lemmatize("production"))
    # print("corpora :", lemmatizer.lemmatize("corpora"))

    df['field_after_change'] = df['field_after_change'].apply(lambda x: preprocess_text(x))

    # old old old old
    # old_df = old_df.dropna()
    # old_df = old_df.reset_index(drop=True)
    # old_df['field_after_change'] = old_df['field_after_change'].apply(lambda x: preprocess_text(x))

    #  some word can be convert to complete word
    # for example:
    # id = identifier
    # no = number
    # net = network
    # doc = document
    # tel = telephone

    """**convert dataset to document , use for Word2vec embedding**"""

    documents = [_text.split() for _text in df.field_after_change]

    # print(documents)

    """**Create w2v model**
    
    
    ***use later on other classification models***
    """

    W2V_SIZE = 100
    W2V_WINDOW = 2
    W2V_MIN_COUNT = 0

    # train model
    w2v_model = gensim.models.Word2Vec(documents,
                                       vector_size=W2V_SIZE,
                                       window=W2V_WINDOW,
                                       min_count=W2V_MIN_COUNT)
    # summarize the loaded model
    # print(w2v_model)
    # summarize vocabulary
    words = list(w2v_model.wv.key_to_index)
    # print(words)
    # access vector for one word
    # print(w2v_model)

    """**split train and test**"""

    TRAIN_SIZE = 0.8
    df_train, df_test = train_test_split(df, test_size=1-TRAIN_SIZE, random_state=11)
    print("TRAIN size:", len(df_train))
    print("TEST size:", len(df_test))
    output += 'Train Size: ' + str(len(df_train)) + '<br>'
    output += 'TEST Size: ' + str(len(df_test)) + '<br><br>'

    """**keras Tokenizer for get dictionary of words and count of words**"""

    tokenizer = Tokenizer()
    tokenizer.fit_on_texts(df_train['field_after_change']) # df_train.field_after_change

    vocab_size = len(tokenizer.word_index) + 1
    print("Total words", vocab_size)



    """**Example of tokenizer and pad_sequences**"""

    SEQUENCE_LENGTH = 10 #  اینو باید کم کنم در حد ۱۰ اینا

    print(df_train.field_after_change[0])
    s = tokenizer.texts_to_sequences([df_train.field_after_change[0]])
    print(s)
    print(tokenizer.sequences_to_texts(s))
    print(df_train.field_after_change[0])
    s_pad = pad_sequences(s,padding='post' ,maxlen=SEQUENCE_LENGTH)
    print(s_pad)

    """**set padding for sentence vector to fix vector size**"""

    x_train = pad_sequences(tokenizer.texts_to_sequences(df_train.field_after_change),padding='post', maxlen=SEQUENCE_LENGTH)
    x_test = pad_sequences(tokenizer.texts_to_sequences(df_test.field_after_change),padding='post', maxlen=SEQUENCE_LENGTH)

    # old old old old
    # old_x_test = pad_sequences(tokenizer.texts_to_sequences(old_df.field_after_change),padding='post', maxlen=SEQUENCE_LENGTH)
    # old_y_test = old_df['is_closed'].values

    tokenizer.word_index.items()

    print(len(x_train))
    print(len(x_test))

    y_train = df_train['is_closed'].values
    y_test = df_test['is_closed'].values

    print("y_train",y_train.shape)
    print("y_test",y_test.shape)

    """**load Embedding model**
    
    ### class for load embedding from pre-train word2vec model for word embedding
    """

    import io

    class EmbeddingModel():
        def __init__(self,words,vectors,n=0,d=0):
          self.words=words
          self.vectors=vectors
          self.n=n
          self.d=d
          self._wordDict={words[i]:vectors[i] for i in range(0,len(words))}
        def getVector(self,word,alt):
          if(word in self._wordDict):
              return self._wordDict[word]
          else:
              return alt
    def loadFromText(fname):
      words = []
      vectors = []
      fin = io.open(fname, 'r', newline='\n', errors='ignore')

      for line in fin:
          tokens = line.rstrip().split(' ')
          words.append(tokens[0])
          vectors.append([float(t) for t in tokens[1:]])
      d = len(vectors[0])
      return EmbeddingModel(words,vectors,0, d)

    use_pre_train_embedding_model = False # wikipedia or gathering news collection in google lab

    if use_pre_train_embedding_model :
      EM = loadFromText("drive/MyDrive/pouya/glove.6B.100d.txt")
      embedding_matrix = np.zeros((vocab_size, EM.d)) # matrix (302 word , 100 dim)
      unkwon_word_count = 0
      uw = []
      for word, i in tokenizer.word_index.items():
        if EM.getVector(word,False):
          embedding_matrix[i] = EM.getVector(word,False)
        else:
            uw.append(word)
            unkwon_word_count += 1
      print(embedding_matrix.shape)
      print(unkwon_word_count)

    else: #else use Word2vec model
      embedding_matrix = np.zeros((vocab_size, W2V_SIZE))
      unkwon_word_count = 0
      uw = []
      for word, i in tokenizer.word_index.items():
        if word in w2v_model.wv:
          embedding_matrix[i] = w2v_model.wv[word]
        else:
            uw.append(word)
            unkwon_word_count += 1
      print(embedding_matrix.shape)
      print(unkwon_word_count)

    print(tokenizer.sequences_to_texts([[9]]))
    print(embedding_matrix[9])

    """# words that be modify using user"""

    print(uw)

    """**Create embedding layer **"""

    print("number of unique words:",vocab_size)
    print("dimentions of embedding vector:", W2V_SIZE)

    # use embedding_matrix pre-train without again training
    embedding_layer = Embedding(vocab_size, W2V_SIZE,weights=[embedding_matrix],input_length=SEQUENCE_LENGTH,trainable=False)

    # use embedding_matrix pre-train with again training
    # embedding_layer = Embedding(vocab_size, W2V_SIZE,weights=[embedding_matrix],input_length=SEQUENCE_LENGTH,trainable=True)

    # use keras embedding with training
    # embedding_layer = Embedding(vocab_size, W2V_SIZE,input_length=SEQUENCE_LENGTH,trainable=True)

    embedding_matrix[0]

    embedding_layer.weights

    # from gensim.scripts.glove2word2vec import glove2word2vec
    # glove_input_file = "drive/MyDrive/pouya/glove.6B.100d.txt"
    word2vec_output_file = "drive/MyDrive/pouya/word2vec.6B.100d.txt"
    # glove2word2vec(glove_input_file, word2vec_output_file)

    from gensim.models import KeyedVectors
    # load the Stanford GloVe model
    filename = word2vec_output_file
    # pre_train_w2v_model = KeyedVectors.load_word2vec_format(filename, binary=False)

    # pre_train_w2v_model.get_vector('discpline')

    print(embedding_layer.input_dim)
    print(embedding_layer.output_dim)



    """# Neural Network Model for classification"""

    # define model
    model = Sequential()

    model.add(embedding_layer)
    # model.add(Bidirectional(LSTM(32, return_sequences = True)))
    model.add(GlobalMaxPool1D())
    model.add(Dense(10, activation="relu"))
    model.add(Dense(10, activation="relu"))
    model.add(Dense(10, activation="relu"))
    model.add(Dropout(0.05))
    model.add(Dense(1, activation='sigmoid'))

    print(model.summary())



    # compile network
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    # fit network
    model.fit(x_train, y_train, epochs=100, batch_size=50 , verbose=2) # 80% of dataset (Train set)

    # evaluate
    loss, acc = model.evaluate(x_test, y_test, verbose=0) # 20% of dataset (Test set)
    print('Test Accuracy: %f' % (acc))
    print('Test Loss: %f' % (loss))

    print(x_test[0])
    print(y_test[0])

    y_pred_test = model.predict(x_test)

    print(y_pred_test[0])

    df_test_x_values = df_test.field_after_change.values
    df_test_y_values = df_test.is_closed.values
    for i in range(len(df_test_x_values)):
      print(f"{i} : {df_test_x_values[i]} , {df_test_y_values[i]}")
      output += f"{i} : <span class='text-dark'>{df_test_x_values[i]}</span>, <span class='{'text-danger' if df_test_y_values[i] == 0 else 'text-success'}'>{df_test_y_values[i]}</span><br>"

    x_test[42].shape[0]

    one_x_test = x_test[42].reshape(1,10)

    y_pred = model.predict_classes(one_x_test)

    y_pred[0]

    y_test[56]

    df_test.iloc[56]

    df_test.head(60)

    # evaluate old dataset
    loss, acc = model.evaluate(x_test, y_test, verbose=0) # 20% of dataset (Test set)
    print('Test Accuracy: %f' % (acc))
    print('Test Loss: %f' % (loss))
    output += '<br><span class="text-success">Test Accuracy:</span> %f' % (acc) + '<br>'
    output += '<span class="text-danger">Test Loss: </span> %f' % (loss) + '<br>'

    return output


def process_data2(data):
    # -*- coding: utf-8 -*-
    """db_field_classification_regression_pouya.ipynb

    Automatically generated by Colaboratory.

    Original file is located at
        https://colab.research.google.com/drive/1_EOxc_uElGzd0MRIrbsMt_DMYvl_tYS_
    """

    # from google.colab import drive
    # drive.mount('/content/drive/')

    # Commented out IPython magic to ensure Python compatibility.
    # Matplot
    import matplotlib.pyplot as plt
    # %matplotlib inline
    import seaborn as sns

    # DataFrame
    import pandas as pd

    # Scikit-learn
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import LabelEncoder
    from sklearn.metrics import confusion_matrix, classification_report, accuracy_score

    # Keras
    from keras.preprocessing.text import Tokenizer
    from keras.preprocessing.sequence import pad_sequences
    from keras.models import Sequential
    from keras.layers import Activation, Dense, Dropout, Embedding, Flatten, MaxPooling1D, LSTM, SpatialDropout1D, \
        GlobalMaxPool1D, Bidirectional, Conv1D
    from keras import utils
    from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint
    from keras.models import load_model

    # Word2vec
    import gensim

    # Utility
    import re
    import numpy as np

    # nltk
    import nltk

    df = data
    print('size of new dataset:', len(df))

    # df

    df = df.filter(['table', 'field_after_change', 'is_closed'])

    ## uncomment when use regression
    # df.is_closed = np.random.uniform(0,1,(len(df),1))

    # df.dtypes

    df = df.dropna()
    df = df.reset_index(drop=True)

    df['is_closed'] = df['is_closed'].apply(int)

    df_open = df[df['is_closed'] == 0]
    df_close = df[df['is_closed'] == 1]

    df_open = df_open.iloc[:len(df_close), ]
    df = pd.concat([df_open, df_close], axis=0)

    # plt.figure(figsize=(8, 6))
    # sns.countplot(df['is_closed'])
    # plt.show()

    # shuffeling
    df = df.sample(frac=1).reset_index(drop=True)

    from nltk.stem.porter import PorterStemmer
    stemmer = PorterStemmer()

    def preprocess_text(sentence):
        # Remove punctuations and numbers
        sentence = re.sub('[^a-zA-Z]', ' ', sentence)

        # Single character removal
        sentence = re.sub(r"\s+[a-zA-Z]\s+", ' ', sentence)

        # Removing multiple spaces
        sentence = re.sub(r'\s+', ' ', sentence)

        # convert to lower case
        tokens = [w.lower() for w in sentence.split(' ')]  # 'hello world' > ['hello' , 'world']
        # stemming (coming -> come)
        # tokens = [stemmer.stem(w) for w in tokens]
        sentence = ' '.join(tokens)
        return sentence

    df['field_after_change'] = df['field_after_change'].apply(lambda x: preprocess_text(x))

    # documents = [_text.split() for _text in df.field_after_change]

    # print(documents)

    # tokenizer = Tokenizer()
    # tokenizer.fit_on_texts(df['field_after_change']) # df_train.field_after_change

    # vocab_size = len(tokenizer.word_index) + 1
    # print("Total words", vocab_size)
    # sequences = tokenizer.texts_to_sequences(df) # Turn text into sequence of numbers

    # tokenizer.index_word = {1:'amin'}

    # tokenizer.word_index = {'amin':1}

    # sequences = tokenizer.texts_to_sequences(['amin'])

    # sequences

    import io

    class EmbeddingModel():
        def __init__(self, words, vectors, d=0):
            self.words = words
            self.vectors = vectors
            self.n = len(words)
            self.d = d
            self._wordDict = {words[i]: vectors[i] for i in range(0, len(words))}
            self._index_word = {i + 1: words[i] for i in range(0, len(words))}
            self._word_index = {words[i]: i + 1 for i in range(0, len(words))}

            # Create a matrix of all embeddings
            self.all_embs = np.stack(self._wordDict.values())
            self.emb_mean = self.all_embs.mean()  # Calculate mean
            self.emb_std = self.all_embs.std()  # Calculate standard deviation

        def getVector(self, word, alt):
            if (word in self._wordDict):
                return self._wordDict[word]
            else:
                return alt

    def loadFromText(fname):
        words = []
        vectors = []
        fin = io.open(fname, 'r', newline='\n', errors='ignore')

        for line in fin:
            tokens = line.rstrip().split(' ')  # -> [word , 1, 3 ,...]
            if len(tokens) == 2:
                continue
            words.append(tokens[0])
            vectors.append([float(t) for t in tokens[1:]])
        d = len(vectors[0])
        return EmbeddingModel(words, vectors, d)

        '''
        w1 1,2,3,4,...
        w2 1,3,34,4,...
        '''

    # use_pre_train_embedding_model = True # wikipedia or gathering news collection in google lab

    # if use_pre_train_embedding_model :
    #   EM = loadFromText("drive/MyDrive/pouya/glove.6B.100d.txt")
    #   # embedding_matrix = np.zeros((vocab_size, EM.d)) # matrix (302 word , 100 dim)
    #   embedding_matrix = np.random.normal(EM.emb_mean, EM.emb_std, (vocab_size, EM.d))
    #   unkwon_word_count = 0
    #   uw = []
    #   for word, i in tokenizer.word_index.items():
    #     if EM.getVector(word,False):
    #       embedding_matrix[i] = EM.getVector(word,False)
    #     else:
    #         uw.append(word)
    #         unkwon_word_count += 1
    #   print(embedding_matrix.shape)
    #   print(unkwon_word_count)
    #   print('unkwon words :', uw)

    EM = loadFromText("./glove.6B.100d.txt")
    # EM = loadFromText("drive/MyDrive/pouya/w2v pretrained/w2v_wiki_100d.txt")

    # #We read the existing text from file in READ mode
    # src=open("drive/MyDrive/pouya/glove.6B.100d.txt","r")
    # fline="400000 100\n"    #Prepending string
    # oline=src.readlines()
    # #Here, we prepend the string we want to on first line
    # oline.insert(0,fline)
    # src.close()

    # #We again open the file in WRITE mode
    # src=open("drive/MyDrive/pouya/glove.6B.100d.txt","w")
    # src.writelines(oline)
    # src.close()

    # !python -m gensim.scripts.word2vec2tensor -i drive/MyDrive/pouya/glove.6B.100d.txt -o drive/MyDrive/pouya/TSVfiles/glove.6B.100d

    print(EM.n, EM.d)

    tokenizer = Tokenizer()
    tokenizer.word_index = EM._word_index
    tokenizer.index_word = EM._index_word

    # sequences = tokenizer.texts_to_sequences(['2'])
    # text = tokenizer.sequences_to_texts([[2]])
    # print(sequences)
    # print(text)

    embedding_matrix = np.zeros((EM.n + 1, EM.d))  # matrix (400000 word , 100 dim)

    for i, word in EM._index_word.items():
        embedding_matrix[i] = EM.getVector(word, False)

    embedding_matrix

    len(embedding_matrix)

    """split train and test dataset

    """

    TRAIN_SIZE = 0.8
    number_of_train_size = int(len(df) * 0.8)
    df_train, df_test = df[:number_of_train_size], df[number_of_train_size:]
    print("TRAIN size:", len(df_train))
    print("TEST size:", len(df_test))
    print(df_test.sort_values(by=["table"], ascending=False))
    SEQUENCE_LENGTH = 10  # [0,0,0,0,0,0,0,0,0,0] maximum number of words in a field is 10

    x_train = pad_sequences(tokenizer.texts_to_sequences(df_train.field_after_change), padding='post',
                            maxlen=SEQUENCE_LENGTH)
    x_test = pad_sequences(tokenizer.texts_to_sequences(df_test.field_after_change), padding='post',
                           maxlen=SEQUENCE_LENGTH)

    # regression
    y_train = df_train['is_closed'].values
    y_test = df_test['is_closed'].values

    print('fields text: ', df_train.iloc[1].field_after_change)
    print('pad sequence as input to NN: ', x_train[1])

    # use embedding_matrix pre-train without again training
    embedding_dim = 100  # W2V_SIZE
    vocab_size = embedding_matrix.shape[0]
    embedding_layer = Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=SEQUENCE_LENGTH,
                                trainable=False)

    """# **ta inja**"""

    # import copy

    # model_num = 102

    # # define model
    # model = Sequential()

    # model.add(embedding_layer)

    # # model.add(GlobalMaxPool1D())

    # model.add(Flatten())

    # model.add(Dense(1, activation="relu"))
    # # model.add(Dense(10, activation="relu"))
    # # model.add(Dense(16, activation="relu"))
    # # model.add(Dense(16, activation="relu"))
    # # model.add(Dense(16, activation="relu"))

    # # model.add(Dropout(0.3))
    # model.add(Dense(1, activation='sigmoid'))
    # # model.add(Dense(1))

    # print(model.summary())

    def create_model2(num_of_hl=2, hl1=8, hl2=2, hl3=2, hl4=2, lstm=0):
        model = Sequential()
        model.add(embedding_layer)

        if lstm:
            model.add(LSTM(lstm))
            # model.add(Bidirectional(LSTM(lstm, return_sequences = True)))

        model.add(Flatten())

        if num_of_hl == 1:
            model.add(Dense(hl1, activation="relu"))

        elif num_of_hl == 2:
            model.add(Dense(hl1, activation="relu"))
            model.add(Dense(hl2, activation="relu"))

        elif num_of_hl == 3:
            model.add(Dense(hl1, activation="relu"))
            model.add(Dense(hl2, activation="relu"))
            model.add(Dense(hl3, activation="relu"))

        elif num_of_hl == 4:
            model.add(Dense(hl1, activation="relu"))
            model.add(Dense(hl2, activation="relu"))
            model.add(Dense(hl3, activation="relu"))
            model.add(Dense(hl4, activation="relu"))

        elif num_of_hl == 5:
            model.add(Dense(hl1, activation="relu"))
            model.add(Dense(hl2, activation="relu"))
            model.add(Dense(hl3, activation="relu"))
            model.add(Dense(hl4, activation="relu"))
            # model.add(Dense(hl5, activation="relu"))

        model.add(Dense(1, activation='sigmoid'))

        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

        return model

    num_of_hl = [2, 3, 4]  # این اعداد یکی یکی میزاری بجای num_of_hl که تعداد لایه ها رو مشخص میکنه
    hl_nodes = [2, 4, 8]  # in adad mishanan jaye hl2,hl3,hl4 ya mituni sefr bezari toye tabe create_model
    lstm = [4, 8]  # in ham age khasti lstm estefade koni mizare 4 ya 8 bejaye lstm toye tabe create_model

    model_num = 200  # esme model moskhas mikone
    model = create_model2(num_of_hl=3, hl2=8, hl3=2, hl4=0, lstm=8)
    model.summary()

    # compile network
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

    ## for regression
    # model.compile(loss='mse', optimizer='adam', metrics=['accuracy','mse'])

    es = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=15)

    # fit neural network
    history = model.fit(x_train, y_train, validation_split=0.2, epochs=100, batch_size=50, verbose=2,
                        callbacks=[])  # 80% of dataset (Train set)


    # Matplot
    import seaborn as sns
    sns.set_theme()

    acc = history.history['accuracy']

    # evaluate
    loss, acc = model.evaluate(x_test, y_test, verbose=1)  # 20% of dataset (Test set)

    print(f'Test Loss: {loss} Test Accuracy: {acc}')

    model.predict(x_test)

    from sklearn.metrics import classification_report, confusion_matrix

    y_prediction = model.predict(x_test, batch_size=1, verbose=2)
    y_pred = y_prediction > 0.5

    df_test_x_values = df_test.field_after_change.values
    table_values = df_test.table.values
    df_test_y_values = df_test.is_closed.values

    print('# | field text | actual value | regression value | predict value according regression value')
    result = []
    tables = []
    final_result = []
    for i in range(len(df_test_x_values)):
        val = 1 if y_prediction[i] >= 0.5 else 0
        x = {
            "table": table_values[i],
            "field": df_test_x_values[i],
            "value": 1 - df_test_y_values[i],
            "regression_value": float("{:.3f}".format(1 - float(y_prediction[i]))),
            "predicted_value": 1 - val
        }
        result.append(x)
        for d in result:
            tables.append(d['table'])
            tables = list(set(tables))

    for i in range(len(tables)):
        expectedResult = [d for d in result if d['table'] == tables[i]]
        summation = 0
        for v in expectedResult:
            summation += float(v['regression_value'])
        avg = summation / len(expectedResult)
        final_result.append({
            'id': i+1,
            'average': float("{:.3f}".format(avg)),
            "table": tables[i],
            'fields': expectedResult
        })

        # print(f"{i} | {df_test_x_values[i]} | {df_test_y_values[i]} | {y_prediction[i]} | {val}")

    return sorted(final_result, key=lambda k: k['average'], reverse=True)

