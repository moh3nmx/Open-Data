# -*- coding: utf-8 -*-
"""dbFieldClassification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EcE_QcWNA8qQ225Nz1sCzbWfhxoHoGsK
"""

# from google.colab import drive
# drive.mount('/content/drive/')

"""# Imports

"""

# Commented out IPython magic to ensure Python compatibility.
# Matplot
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns

# DataFrame
import pandas as pd

# Scikit-learn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score

# Keras
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Activation, Dense, Dropout, Embedding, Flatten , MaxPooling1D, LSTM, SpatialDropout1D,GlobalMaxPool1D,Bidirectional
from keras import utils
from keras.callbacks import ReduceLROnPlateau, EarlyStopping , ModelCheckpoint
from keras.models import load_model

# Word2vec
import gensim

# Utility
import re
import numpy as np

# nltk
import nltk

#----------- if you need wordnet run follow line
# nltk.download('wordnet')


def process_data(data):
    output = ''
    # old_df = pd.read_csv("dataset.csv")
    df = data

    # print('size of old dataset:',len(old_df))
    print('size of new dataset:',len(df))
    output += 'size of new dataset: ' + str(len(df)) + '<br><br>'

    df.head(60)

    df = df.filter(['field_after_change','is_closed'])
    # old_df = old_df.filter(['field_after_change','is_closed'])

    # df_temp = df[df.is_closed == 1]
    # df_temp.to_csv('drive/MyDrive/pouya/fields_are_just_closed.csv')

    """### remove nan from dataset"""

    df = df.dropna()
    df = df.reset_index(drop=True)

    df['is_closed'].sum()

    #shuffeling
    df = df.sample(frac=1).reset_index(drop=True)

    # plt.figure(figsize = (8, 6))
    # sns.countplot(df['is_closed'])
    # plt.show()

    df

    """## text cleaning and preprocessing"""

    from nltk.stem.porter import PorterStemmer
    stemmer = PorterStemmer()

    def preprocess_text(sentence):
      # Remove punctuations and numbers
      sentence = re.sub('[^a-zA-Z]', ' ', sentence)

      # Single character removal
      sentence = re.sub(r"\s+[a-zA-Z]\s+", ' ', sentence)

      # Removing multiple spaces
      sentence = re.sub(r'\s+', ' ', sentence)

      # convert to lower case
      tokens = [w.lower() for w in sentence.split(' ')]
      # stemming (coming -> come)
      # tokens = [stemmer.stem(w) for w in tokens]
      sentence = ' '.join(tokens)
      return sentence

    # from nltk.stem import WordNetLemmatizer

    # lemmatizer = WordNetLemmatizer()

    # print("rocks :", lemmatizer.lemmatize("production"))
    # print("corpora :", lemmatizer.lemmatize("corpora"))

    df['field_after_change'] = df['field_after_change'].apply(lambda x: preprocess_text(x))

    # old old old old
    # old_df = old_df.dropna()
    # old_df = old_df.reset_index(drop=True)
    # old_df['field_after_change'] = old_df['field_after_change'].apply(lambda x: preprocess_text(x))

    #  some word can be convert to complete word
    # for example:
    # id = identifier
    # no = number
    # net = network
    # doc = document
    # tel = telephone

    """**convert dataset to document , use for Word2vec embedding**"""

    documents = [_text.split() for _text in df.field_after_change]

    # print(documents)

    """**Create w2v model**
    
    
    ***use later on other classification models***
    """

    W2V_SIZE = 100
    W2V_WINDOW = 2
    W2V_MIN_COUNT = 0

    # train model
    w2v_model = gensim.models.Word2Vec(documents,
                                       vector_size=W2V_SIZE,
                                       window=W2V_WINDOW,
                                       min_count=W2V_MIN_COUNT)
    # summarize the loaded model
    # print(w2v_model)
    # summarize vocabulary
    words = list(w2v_model.wv.key_to_index)
    # print(words)
    # access vector for one word
    # print(w2v_model)

    """**split train and test**"""

    TRAIN_SIZE = 0.8
    df_train, df_test = train_test_split(df, test_size=1-TRAIN_SIZE, random_state=11)
    print("TRAIN size:", len(df_train))
    print("TEST size:", len(df_test))
    output += 'Train Size: ' + str(len(df_train)) + '<br>'
    output += 'TEST Size: ' + str(len(df_test)) + '<br><br>'

    """**keras Tokenizer for get dictionary of words and count of words**"""

    tokenizer = Tokenizer()
    tokenizer.fit_on_texts(df_train['field_after_change']) # df_train.field_after_change

    vocab_size = len(tokenizer.word_index) + 1
    print("Total words", vocab_size)



    """**Example of tokenizer and pad_sequences**"""

    SEQUENCE_LENGTH = 10 #  اینو باید کم کنم در حد ۱۰ اینا

    print(df_train.field_after_change[0])
    s = tokenizer.texts_to_sequences([df_train.field_after_change[0]])
    print(s)
    print(tokenizer.sequences_to_texts(s))
    print(df_train.field_after_change[0])
    s_pad = pad_sequences(s,padding='post' ,maxlen=SEQUENCE_LENGTH)
    print(s_pad)

    """**set padding for sentence vector to fix vector size**"""

    x_train = pad_sequences(tokenizer.texts_to_sequences(df_train.field_after_change),padding='post', maxlen=SEQUENCE_LENGTH)
    x_test = pad_sequences(tokenizer.texts_to_sequences(df_test.field_after_change),padding='post', maxlen=SEQUENCE_LENGTH)

    # old old old old
    # old_x_test = pad_sequences(tokenizer.texts_to_sequences(old_df.field_after_change),padding='post', maxlen=SEQUENCE_LENGTH)
    # old_y_test = old_df['is_closed'].values

    tokenizer.word_index.items()

    print(len(x_train))
    print(len(x_test))

    y_train = df_train['is_closed'].values
    y_test = df_test['is_closed'].values

    print("y_train",y_train.shape)
    print("y_test",y_test.shape)

    """**load Embedding model**
    
    ### class for load embedding from pre-train word2vec model for word embedding
    """

    import io

    class EmbeddingModel():
        def __init__(self,words,vectors,n=0,d=0):
          self.words=words
          self.vectors=vectors
          self.n=n
          self.d=d
          self._wordDict={words[i]:vectors[i] for i in range(0,len(words))}
        def getVector(self,word,alt):
          if(word in self._wordDict):
              return self._wordDict[word]
          else:
              return alt
    def loadFromText(fname):
      words = []
      vectors = []
      fin = io.open(fname, 'r', newline='\n', errors='ignore')

      for line in fin:
          tokens = line.rstrip().split(' ')
          words.append(tokens[0])
          vectors.append([float(t) for t in tokens[1:]])
      d = len(vectors[0])
      return EmbeddingModel(words,vectors,0, d)

    use_pre_train_embedding_model = False # wikipedia or gathering news collection in google lab

    if use_pre_train_embedding_model :
      EM = loadFromText("drive/MyDrive/pouya/glove.6B.100d.txt")
      embedding_matrix = np.zeros((vocab_size, EM.d)) # matrix (302 word , 100 dim)
      unkwon_word_count = 0
      uw = []
      for word, i in tokenizer.word_index.items():
        if EM.getVector(word,False):
          embedding_matrix[i] = EM.getVector(word,False)
        else:
            uw.append(word)
            unkwon_word_count += 1
      print(embedding_matrix.shape)
      print(unkwon_word_count)

    else: #else use Word2vec model
      embedding_matrix = np.zeros((vocab_size, W2V_SIZE))
      unkwon_word_count = 0
      uw = []
      for word, i in tokenizer.word_index.items():
        if word in w2v_model.wv:
          embedding_matrix[i] = w2v_model.wv[word]
        else:
            uw.append(word)
            unkwon_word_count += 1
      print(embedding_matrix.shape)
      print(unkwon_word_count)

    print(tokenizer.sequences_to_texts([[9]]))
    print(embedding_matrix[9])

    """# words that be modify using user"""

    print(uw)

    """**Create embedding layer **"""

    print("number of unique words:",vocab_size)
    print("dimentions of embedding vector:", W2V_SIZE)

    # use embedding_matrix pre-train without again training
    embedding_layer = Embedding(vocab_size, W2V_SIZE,weights=[embedding_matrix],input_length=SEQUENCE_LENGTH,trainable=False)

    # use embedding_matrix pre-train with again training
    # embedding_layer = Embedding(vocab_size, W2V_SIZE,weights=[embedding_matrix],input_length=SEQUENCE_LENGTH,trainable=True)

    # use keras embedding with training
    # embedding_layer = Embedding(vocab_size, W2V_SIZE,input_length=SEQUENCE_LENGTH,trainable=True)

    embedding_matrix[0]

    embedding_layer.weights

    # from gensim.scripts.glove2word2vec import glove2word2vec
    # glove_input_file = "drive/MyDrive/pouya/glove.6B.100d.txt"
    word2vec_output_file = "drive/MyDrive/pouya/word2vec.6B.100d.txt"
    # glove2word2vec(glove_input_file, word2vec_output_file)

    from gensim.models import KeyedVectors
    # load the Stanford GloVe model
    filename = word2vec_output_file
    # pre_train_w2v_model = KeyedVectors.load_word2vec_format(filename, binary=False)

    # pre_train_w2v_model.get_vector('discpline')

    print(embedding_layer.input_dim)
    print(embedding_layer.output_dim)



    """# Neural Network Model for classification"""

    # define model
    model = Sequential()

    model.add(embedding_layer)
    # model.add(Bidirectional(LSTM(32, return_sequences = True)))
    model.add(GlobalMaxPool1D())
    model.add(Dense(10, activation="relu"))
    model.add(Dense(10, activation="relu"))
    model.add(Dense(10, activation="relu"))
    model.add(Dropout(0.05))
    model.add(Dense(1, activation='sigmoid'))

    print(model.summary())



    # compile network
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    # fit network
    model.fit(x_train, y_train, epochs=100, batch_size=50 , verbose=2) # 80% of dataset (Train set)

    # evaluate
    loss, acc = model.evaluate(x_test, y_test, verbose=0) # 20% of dataset (Test set)
    print('Test Accuracy: %f' % (acc))
    print('Test Loss: %f' % (loss))

    print(x_test[0])
    print(y_test[0])

    y_pred_test = model.predict(x_test)

    print(y_pred_test[0])

    df_test_x_values = df_test.field_after_change.values
    df_test_y_values = df_test.is_closed.values
    for i in range(len(df_test_x_values)):
      print(f"{i} : {df_test_x_values[i]} , {df_test_y_values[i]}")
      output += f"{i} : <span class='text-dark'>{df_test_x_values[i]}</span>, <span class='{'text-danger' if df_test_y_values[i] == 0 else 'text-success'}'>{df_test_y_values[i]}</span><br>"

    x_test[42].shape[0]

    one_x_test = x_test[42].reshape(1,10)

    y_pred = model.predict_classes(one_x_test)

    y_pred[0]

    y_test[56]

    df_test.iloc[56]

    df_test.head(60)

    # evaluate old dataset
    loss, acc = model.evaluate(x_test, y_test, verbose=0) # 20% of dataset (Test set)
    print('Test Accuracy: %f' % (acc))
    print('Test Loss: %f' % (loss))
    output += '<br><span class="text-success">Test Accuracy:</span> %f' % (acc) + '<br>'
    output += '<span class="text-danger">Test Loss: </span> %f' % (loss) + '<br>'

    return output
